{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9c8eaa-46a4-494a-926c-bea16dfcf6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanfordnlp\n",
      "  Obtaining dependency information for stanfordnlp from https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading stanfordnlp-0.2.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from stanfordnlp) (1.24.4)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from stanfordnlp) (4.23.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from stanfordnlp) (2.31.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from stanfordnlp) (2.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from stanfordnlp) (4.66.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->stanfordnlp) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.0.0->stanfordnlp) (68.1.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.0.0->stanfordnlp) (0.41.2)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.0.0->stanfordnlp) (3.27.5)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.0.0->stanfordnlp) (16.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->stanfordnlp) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->stanfordnlp) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->stanfordnlp) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->stanfordnlp) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->stanfordnlp) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.0.0->stanfordnlp) (1.3.0)\n",
      "Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: stanfordnlp\n",
      "Successfully installed stanfordnlp-0.2.0\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.11/site-packages (0.42.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install stanfordnlp\n",
    "!pip install jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef977cf-d035-4f72-aff9-16a8ba415246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"zh_gsd\" for language \"zh\".\n",
      "Would you like to download the models for: zh_gsd now? (Y/n)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default download directory: /home/jovyan/stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading models for: zh_gsd\n",
      "Download location: /home/jovyan/stanfordnlp_resources/zh_gsd_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234M/234M [00:43<00:00, 5.44MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: /home/jovyan/stanfordnlp_resources/zh_gsd_models.zip\n",
      "Extracting models file for: zh_gsd\n",
      "Cleaning up...Done.\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/jovyan/stanfordnlp_resources/zh_gsd_models/zh_gsd_tokenizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/jovyan/stanfordnlp_resources/zh_gsd_models/zh_gsd_tagger.pt', 'pretrain_path': '/home/jovyan/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/jovyan/stanfordnlp_resources/zh_gsd_models/zh_gsd_lemmatizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/jovyan/stanfordnlp_resources/zh_gsd_models/zh_gsd_parser.pt', 'pretrain_path': '/home/jovyan/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "# 下载中文模型\n",
    "stanfordnlp.download('zh')\n",
    "nlp = stanfordnlp.Pipeline(lang='zh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f2bb2b-dcc6-4c51-9229-6285dab2e8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.526 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index_select(): Expected dtype int32 or int64 for index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m comments:\n\u001b[1;32m     23\u001b[0m     words \u001b[38;5;241m=\u001b[39m jieba\u001b[38;5;241m.\u001b[39mlcut(comment)\n\u001b[0;32m---> 24\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     entity_words \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msentences:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/stanfordnlp/pipeline/core.py:176\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    175\u001b[0m     doc \u001b[38;5;241m=\u001b[39m Document(doc)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/stanfordnlp/pipeline/core.py:170\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor_names:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocessor_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m doc\u001b[38;5;241m.\u001b[39mload_annotations()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/stanfordnlp/pipeline/lemma_processor.py:66\u001b[0m, in \u001b[0;36mLemmaProcessor.process\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     64\u001b[0m edits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq2seq_batch):\n\u001b[0;32m---> 66\u001b[0m     ps, es \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ps\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m es \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/stanfordnlp/models/lemma/trainer.py:88\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, batch, beam_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     87\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m preds, edit_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m pred_seqs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munmap(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m preds] \u001b[38;5;66;03m# unmap to tokens\u001b[39;00m\n\u001b[1;32m     90\u001b[0m pred_seqs \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mprune_decoded_seqs(pred_seqs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/stanfordnlp/models/common/seq2seq_model.py:210\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict\u001b[0;34m(self, src, src_mask, pos, beam_size)\u001b[0m\n\u001b[1;32m    208\u001b[0m         done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [b]\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# update beam state\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_origin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(done) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/stanfordnlp/models/common/seq2seq_model.py:193\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict.<locals>.update_state\u001b[0;34m(states, idx, positions, beam_size)\u001b[0m\n\u001b[1;32m    191\u001b[0m br, d \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    192\u001b[0m s \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(beam_size, br \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m beam_size, d)[:,idx]\n\u001b[0;32m--> 193\u001b[0m s\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy_(\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index_select(): Expected dtype int32 or int64 for index"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import stanfordnlp\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 读取 Excel 文件\n",
    "file_path = 'Blossoms_DouBan_Review.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 添加自定义词汇\n",
    "custom_words = ['和平饭店', '至真园', '泡饭', '排骨年糕', '蓝鱼秃肺拼海参', '红烧划水', '定胜糕', '油墩子', '川沙鸡爪', '牛河', '黄鱼面', '饭团', '鹤针', '舟王炒饭', '炎王蛇', '三文鱼', '鲶鱼', '茶叶蛋', '火锅', '辣肉面', '包子']\n",
    "for word in custom_words:\n",
    "    jieba.add_word(word)\n",
    "\n",
    "# 获取评论内容列，并将其转换为字符串类型\n",
    "comments = df['内容'].astype(str)\n",
    "\n",
    "# 文本预处理：分词\n",
    "filtered_words = []\n",
    "for comment in comments:\n",
    "    words = jieba.lcut(comment)\n",
    "    doc = nlp(comment)\n",
    "    entity_words = []\n",
    "    for sentence in doc.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            if token.ner in ['PERSON', 'ORG']:  # 过滤人名和组织名\n",
    "                entity_words.append(token.words[0].text)\n",
    "    filtered_words.extend([word for word in words if word not in entity_words])\n",
    "\n",
    "# 读取扩展停用词库文件\n",
    "with open('cn_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    file_stop_words = set(f.read().splitlines())\n",
    "\n",
    "# 定义并扩展停用词库\n",
    "custom_stop_words = set([\n",
    "    '的', '了', '和', '是', '我', '也', '在', '有', '就', '不', '人', '都', '这个', '上', '很', '你', '他', '她',\n",
    "    '它', '我们', '他们', '她们', '自己', '所以', '因为', '这样', '这里', '那里', '什么', '但是', '如果', '那么',\n",
    "    '还是', '虽然', '不过', '而且', '并且', '关于', '其中', '甚至', '一些', '还有', '或者', '所以', '其实', '另外', '其实',\n",
    "    '以及', '就是', '与', '就', '最', '已经', '非常', '那么', '而', '并', '还', '其中', '所有', '所有的', '还有', '只是',\n",
    "    '几乎', '其他', '而且', '但', '呢', '却', '哇', '哈', '吧', '啊', '的', '嘞', '啦', '吗', '呀'\n",
    "])\n",
    "\n",
    "# 将自定义停用词和文件中的停用词合并\n",
    "stop_words = custom_stop_words.union(file_stop_words)\n",
    "\n",
    "# 过滤停用词和长度小于2的词语\n",
    "filtered_words = [word for word in filtered_words if word not in stop_words and len(word) > 1]\n",
    "\n",
    "# 统计词频\n",
    "word_freq = Counter(filtered_words)\n",
    "common_words = word_freq.most_common(30)\n",
    "print(common_words)\n",
    "\n",
    "# 生成词云\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(common_words))\n",
    "\n",
    "# 显示词云图\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
